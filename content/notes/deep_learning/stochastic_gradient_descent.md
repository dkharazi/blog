---
title: "Stochastic Gradient Descent"
draft: true
weight: "14"
katex: true
---

### Motivating Stochastic Gradient Descent
- Stochastic gradient descent applied to nonconvex loss functions has no such convergence guarantee and is sensitive to the values of the initial parameters
- For feedforward neural networks, it is important to initialize all weights to small random values
- The biases may be initialized to zero or to small positive values
- Roughly, it suffices to understand that the training algorithm is almost always based on using the gradient to descend the cost function

### Defining Stochastic Gradient Descent

---

### tldr

---

### References
- [4.3 Gradient-Based Optimization](http://www.deeplearningbook.org/contents/numerical.html)
- [5.9 Stochastic Gradient Descent](http://www.deeplearningbook.org/contents/ml.html)
- [6.2 Gradient-Based Learning](http://www.deeplearningbook.org/contents/mlp.html#pf6)
